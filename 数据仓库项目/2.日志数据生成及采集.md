# 日志的生成及采集


## 日志的生成
日志的生成使用官方包中提供的日志生成脚本
`log-collector-1.0-SNAPSHOT-jar-with-dependencies`.上传到hadoop101以及hadoop102。

## 日志的采集

日志的采集发生在hadoop101以及hadoop102。日志的采集使用脚本生产，flume读取日志，生产到kafka，继续由消费flume从kafka进行消费，最后写入的hdfs，为采集日志需要安装和配置以下软件，安装都只需要在对应的机器上解压即可，主要在于配置。

### zookeeper

- 安装后在zookper的安装目录下建立zkData,并在zkData目录下建立myid文件，三台机器的myid需要配置不同，分别配置为101，102，103。myid的配置不同,leader也会不同
- 配置zookeeper安装目录/conf下的先赋值zoo.cfg.templete 为zoo.cfg,修改。再zoo.cfg下增加server.101=hadoop101:2888:3888,server.102=hadoop102:2888:3888,server.103=hadoop103:2888:3888。zookeeper再启动时会读取该值。

### hadoop

hadoop的安装和配置不多说。在实际项目中可以配置多目录，由于使用虚拟机只用了一个磁盘，因此也没有配置多目录。

### Flume

由于三台机器上都需要使用flume，因此三台机器都要安装。flume安装即可使用。

#### 生产flume的配置

1. 为实现实时监控日志数据，同时将日志数据生产到kafka之中，使用TAILDIR对日志生成的文件进行监控。使用kafka channel将数据生产到kafka。
2. 在讲数据写入时，将日志分为event日志和start日志，分别写入kafka中的不同主题，因此需配置选择器。选择器使用multiplexing，按照header的不同写入不同的channel之中
3. 为了给不同类型的日志不同的header，需要配置自定义拦截器。
4. 为了对数据清洗，需要再次配置自定义拦截器


```conf
a1.sources=r1
a1.channels=c1 c2

# configure source
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /opt/module/flume/test/log_position.json
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /tmp/logs/app.+
a1.sources.r1.fileHeader = true
a1.sources.r1.channels = c1 c2

#interceptor
a1.sources.r1.interceptors =  i1 i2
a1.sources.r1.interceptors.i1.type = cn.lpc.flume.interceptor.LogETLInterceptor$Builder
a1.sources.r1.interceptors.i2.type = cn.lpc.flume.interceptor.LogTypeInterceptor$Builder

a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = topic
a1.sources.r1.selector.mapping.topic_start = c1
a1.sources.r1.selector.mapping.topic_event = c2

# configure channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = hadoop101:9092,hadoop102:9092,hadoop103:9092
a1.channels.c1.kafka.topic = topic_start
a1.channels.c1.parseAsFlumeEvent = false
a1.channels.c1.kafka.consumer.group.id = flume-consumer

a1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c2.kafka.bootstrap.servers = hadoop101:9092,hadoop102:9092,hadoop103:9092
a1.channels.c2.kafka.topic = topic_event
a1.channels.c2.parseAsFlumeEvent = false
a1.channels.c2.kafka.consumer.group.id = flume-consumer
```

```java
package cn.lpc.MyInteceptor;
import org.apache.commons.lang.StringUtils;
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;
import org.apache.flume.interceptor.StaticInterceptor;

import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

/**
 * 项目名：Inteceptor
 * 描述：自定义拦截器
 *实现两个功能，
 * 1. 拦截不合法的json对象
 * 2. 将两种json对象放入赋值不同的header，从而实现进入不容的channel
 * event日志 1576233590491|{"cm":{"ln":"-67.5","sv":"V2.9.7","os":"8.1.4","g":"5P0CPYGD@gmail.com"}
 * start日志{"action":"1","ar":"MX","ba":"HTC","detail":"201","en":"start","entry":"3","extend1":"","g":"TTBSTV99@gmail.com","hw":"640*1136","l":"es","la":"25.2","ln":"-119.3","loading_time":"14","md":"HTC-11","mid":"999","nw":"WIFI","open_ad_type":"2","os":"8.2.8","sr":"H","sv":"V2.0.0","t":"1576230687961","uid":"999","vc":"16","vn":"1.0.7"}
 *
 *
 * @author : Lpc
 * @date : 2019-12-13 19:06
 **/
public class MyInterceptor implements Interceptor {
    //cn.lpc.MyInteceptor.MyInterceptor￥
    ArrayList<Event> newEvents;

    @Override
    public void initialize() {
        newEvents=new ArrayList<Event>();
    }
    @Override
    public Event intercept(Event event) {
        byte[] body = event.getBody();
        String log = new String(body, Charset.forName("UTF-8"));
        Map<String, String> headers = event.getHeaders();
        if (StringUtils.isBlank(log)){
            return null;
        }

        if (log.contains("start")){
           headers.put("topic","start");
           // CKUtils为自定义的对于event进行验证的工具类。如果合法放回原event，如不合法返回null
           return CKUtils.ckStart(event);
        }else {
            headers.put("topic","event");
            return CKUtils.ckEvent(event);
        }
    }
    @Override
    public List<Event> intercept(List<Event> events) {
        for (Event event : events) {
           if (intercept(event)!=null){
               newEvents.add(event);
           }
        }

        return newEvents;
    }
    @Override
    public void close() {

    }
    //cn.lpc.MyInteceptor.MyInterceptor$Builder
    //cn.lpc.MyInteceptor.MyInterceptor$Builder
    public static class Builder implements Interceptor.Builder {

        @Override
        public Interceptor build() {
            return new MyInterceptor();
        }

        @Override
        public void configure(Context context) {

        }
    }
}

```
```java
import org.apache.commons.lang.math.NumberUtils;
import org.apache.flume.Event;
import java.nio.charset.Charset;

/**
 * 项目名：Inteceptor
 * 描述：用于解析一个json对象判断是否合法
 *
 * @author : Lpc
 * @date : 2019-12-13 19:11
 **/
public class CKUtils {

    public static   Event ckStart(Event e){
        String body = new String(e.getBody(), Charset.forName("utf-8"));

        if (body.startsWith("{") && body.endsWith("}")){
            return e;
        }else {
            return null;
        }

    }
    public static Event ckEvent(Event e){
        String body = new String(e.getBody(), Charset.forName("utf-8"));
        String[] split = body.split("\\|");

        if (!(NumberUtils.isDigits(split[0]) && split[0].trim().length()==13)){
            return null;
        }


        if (split[1].startsWith("{") && split[1].endsWith("}") ){
            return e;
        }else {
            return null;
        }

    }
}
```

在一切配置好后用将配置文件命名为file-flume-kafka.conf，在hadoop102和Hadoop101上启动Kafka
```shell
~hadoop101 "nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE > /dev/null 2>&1 &"
~hadoop102 "nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE > /dev/null 2>&1 &"
```

第一个参数`nohup`表示不挂起，在退出账号后该进程将继续允许，
最后一个参数 `& `表示后台运行。
`/dev/null` 表示将输出写入到黑洞

#### 消费flume的配置

消费者flume从kafka中读取数据，使用filechannel，最后写入到hdfs上

```conf
## 组件
a1.sources=r1 r2
a1.channels=c1 c2
a1.sinks=k1 k2

## source1
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = hadoop101:9092,hadoop102:9092,hadoop103:9092
a1.sources.r1.kafka.topics=topic_start

## source2
a1.sources.r2.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r2.batchSize = 5000
a1.sources.r2.batchDurationMillis = 2000
a1.sources.r2.kafka.bootstrap.servers = hadoop101:9092,hadoop102:9092,hadoop103:9092
a1.sources.r2.kafka.topics=topic_event

## channel1
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/module/flume/checkpoint/behavior1
a1.channels.c1.dataDirs = /opt/module/flume/data/behavior1/
a1.channels.c1.maxFileSize = 2146435071
a1.channels.c1.capacity = 1000000
a1.channels.c1.keep-alive = 6

## channel2
a1.channels.c2.type = file
a1.channels.c2.checkpointDir = /opt/module/flume/checkpoint/behavior2
a1.channels.c2.dataDirs = /opt/module/flume/data/behavior2/
a1.channels.c2.maxFileSize = 2146435071
a1.channels.c2.capacity = 1000000
a1.channels.c2.keep-alive = 6

## sink1
## hdfs sink also buckets/partitions data by attributes like timestamp or machine where the event originated. 如果提供timestamp，就可以使用%Y之类的方法。
## 如果没有配置timestamp将使用flume所在的本地时间作为timestamp
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_start/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = logstart-
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = second

##sink2
a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.path = /origin_data/gmall/log/topic_event/%Y-%m-%d
a1.sinks.k2.hdfs.filePrefix = logevent-
a1.sinks.k2.hdfs.round = true
a1.sinks.k2.hdfs.roundValue = 10
a1.sinks.k2.hdfs.roundUnit = second

## 不要产生大量小文件
## rollInterval设置为0后将不按照时间滚动
a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

a1.sinks.k2.hdfs.rollInterval = 10
a1.sinks.k2.hdfs.rollSize = 134217728
a1.sinks.k2.hdfs.rollCount = 0

## 控制输出文件是原生文件。
a1.sinks.k1.hdfs.fileType = CompressedStream 
a1.sinks.k2.hdfs.fileType = CompressedStream 

## 配置压缩
a1.sinks.k1.hdfs.codeC = lzop
a1.sinks.k2.hdfs.codeC = lzop

## 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1

a1.sources.r2.channels = c2
a1.sinks.k2.channel= c2
```

配置后` hadoop103 "nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/kafka-flume-hdfs.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume/log.txt   2>&1 &"`
在haodop103上启动f2.
#### kafka的安装配置

安装后配置server.properties文件
```conf
#broker的全局唯一编号，不能重复
broker.id=0
#删除topic功能使能
delete.topic.enable=true
#处理网络请求的线程数量
num.network.threads=3
#用来处理磁盘IO的现成数量
num.io.threads=8
#发送套接字的缓冲区大小
socket.send.buffer.bytes=102400
#接收套接字的缓冲区大小
socket.receive.buffer.bytes=102400
#请求套接字的缓冲区大小
socket.request.max.bytes=104857600
#kafka运行日志存放的路径	
log.dirs=/opt/module/kafka/logs
#topic在当前broker上的分区个数
num.partitions=1
#用来恢复和清理data下数据的线程数量
num.recovery.threads.per.data.dir=1
#segment文件保留的最长时间，超时将被删除
log.retention.hours=168
#配置连接Zookeeper集群地址
zookeeper.connect=hadoop101:2181,hadoop102:2181,hadoop103:2181
```

创建topic_start,和topic_event两个主题，注意与flume中配置的要一致。
