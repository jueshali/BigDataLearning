# 数据的读取和保存

## Text文件

sc.textFile("")，用hadoop的TextInputFormat读入数据

## json文件

要求读入的json文件一行是一个json.因为json一开始就是使用的textFile按行读取。非常麻烦。读入时可以使用scala.util.parsing.json.JSON工具类

```scala
object Text {
    def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("CreateRDD")
        val sc = new SparkContext(conf)
        var rdd = sc.parallelize(Array((1, "a"), (1, "b"), (2, "c"),(4,"d")))
        val rddJson: RDD[String] = sc.textFile("a.txt")
        rddJson.map(JSON.parseFull)
        rdd.saveAsTextFile("c:/")
    }
}
```

## MySQL
```scala
object Sql {
    def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("CreateRDD")
        val sc = new SparkContext(conf)
        //new JdbcRDD()
        /*
        class JdbcRDD[T: ClassTag](
        sc: SparkContext,
        getConnection: () => Connection,
        sql: String,
        lowerBound: Long, // 上限
        upperBound: Long, // 下限=>用于分区
        numPartitions: Int,
        mapRow: (ResultSet) => T = JdbcRDD.resultSetToObjectArray _)
        extends RDD[T](sc, Nil) with Logging {
        读数据New一个jdbcRDD然后collect即可
        存数据的直接用jdbc的存数据即可。
        用ForeachPartition()，每个分区只要建立一次连接
      */

    }
}


```

## HBase




