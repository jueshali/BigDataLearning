# RDD的运算

## RDD运算的步骤

对于RDD的运算是以下一个步骤。
1. 创建RDD对象
2. 经过一系列的transformations对RDD进行转换
3. 调用action触发RDD的计算（延迟计算）
4. 要使用spark，开发者需要编写driver用于调度运行Worker

## RDD的创建

在Spark中创建RDD的方式可以分为3种：
- 从集合创建RDD
    `val rdd1: RDD[Int] = sc.parallelize(Array(30,50,70,60,10,20,30,30))`
- 从外部存储创建RDD
  `sc.textFile("c://a.txt")`
- 从其他RDD转换
  `val rdd2: RDD[Int] = rdd1.map(x => x)`

## RDD的转换
经过transformations对RDD进行转换，这些算子可以分为单value的和多value的，类似于

### 单value，都是窄连接

#### map(func)

类似于scala中map，对RDD中的每一个元素进行映射，窄连接

```java
 def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("CreateRDD")
        val sc = new SparkContext(conf)
        // 默认的切片数为前面的核心数
        val rdd1: RDD[Int] = sc.parallelize(Array(1, 3, 4))
        rdd1.map(s=>{math.sqrt(s)}).collect().foreach(println)
    }
```

#### mapPartitions(func)

对每一个分区中的元素做射

```java
 def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("CreateRDD")
        val sc = new SparkContext(conf)
        // 默认的切片数为前面的核心数
        val rdd1: RDD[Int] = sc.parallelize(Array(30,50,70,60,10,20))
        rdd1.mapPartitions(it=>it.map(math.pow(_,2))).collect().foreach(println)
        rdd1.map(
            // 建立到mysql的连接
            // 从mysql读数据
            // 一个rdd钟的每个元素操作时做一次操作
            t=>t
        )
        rdd1.mapPartitions(it=>{
                // 建立到mysql的连接
                // 从mysql读数据
            // 一个rdd钟的每个每个操作时做一次操作
            // 原来数据处理完才能释放原来的数据，导致oom
            // 效率比较高
            it
        })
    }
```

#### mapPartitionsWithIndex(func)
对每一个分区中的元素做一次映射，最外层的映射是从集合到集合，里面还有一个映射是对集合的操作，调用的map是scala中的map.

```java
  def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("CreateRDD")
        val sc = new SparkContext(conf)
        // 默认的切片数为前面的核心数
        val rdd1: RDD[Int] = sc.parallelize(Array(30,50,70,60,10,20))
        rdd1.mapPartitions(it=>it.map(math.pow(_,2))).collect().foreach(println)
        rdd1.map(
            // 建立到mysql的连接
            // 从mysql读数据
            // 一个rdd钟的每个元素操作时做一次操作
            t=>t
        )
        rdd1.mapPartitions(it=>{
                // 建立到mysql的连接
                // 从mysql读数据
            // 一个rdd钟的每个每个操作时做一次操作
            // 原来数据处理完才能释放原来的数据，导致oom
            // 效率比较高
            it
        })
    }
```

#### flatMap(func)
类似于scala中的flatMap

```java
 def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("CreateRDD")
        val sc = new SparkContext(conf)
        // 默认的切片数为前面的核心数
        val rdd1: RDD[Int] = sc.parallelize(Array(30,50,70,60,10,20))
        rdd1.flatMap(x=>Array(x,x)).collect().foreach(println)
        println("------------")
        // 使用flatMap实现过滤
        rdd1.flatMap{
            case x if x>50 =>Array(x)
            case _ => Array[Int]()
        }.collect().foreach(println)
        sc.stop()
    }
```

#### glom()

将一个分区中的元素用以个array存储

#### groupBy(func)
分区，注意的是分区需要shuffle性能低，二是分区之后每个区中间的数据的顺序不确定，其底层调用的依旧是groupBykey

```java
def groupBy[K](f: T => K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null)
      : RDD[(K, Iterable[T])] = withScope {
    val cleanF = sc.clean(f)
    this.map(t => (cleanF(t), t)).groupByKey(p)
  }
```

```java
    def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("CreateRDD")
        val sc = new SparkContext(conf)
        //
        val rdd1: RDD[Int] = sc.parallelize(Array(3,5,70,60,10,20))
        // groupBy需要shuffle,分完组之后为之前的分区数。
        // shuffle会降低spark的性能，尽量不用
        rdd1.groupBy(x=>x%2).collect().foreach(println)
    }
```


#### filter(func)
类似于scala中的过滤器

#### sample()
抽样器，在大数据场景下可能需要对部分数据进行抽样运算。\
withReplacement: Boolean， 抽样时是否放回\
fraction: Double,   抽样的比例\
seed    随机数生成种子\
note:如果抽样时可放回则抽样比例可以大于1，如果不可放回则抽样比例应该在0到1之间。

```java
def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("CreateRDD")
        val sc = new SparkContext(conf)
        val rdd1: RDD[Int] = sc.parallelize(Array(30,50,70,60,10,70,60,10,70,60,10,70,60,10,70,60,10,70,60,1070,60,10,70,60,10))
        // fraction 必须大于0小于1
        // withReplacement表示抽取后是否放回
        rdd1.sample(true,0.1).collect().foreach(println)
    }

```
#### distinct
对数据进行去重操作
```java
  def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("CreateRDD")
        val sc = new SparkContext(conf)
        val rdd1: RDD[Int] = sc.parallelize(Array(30,50,70,60,10,20,30,30))
        // 去重
        rdd1.distinct().collect.foreach(println)
        //用Ordering进行比较
    }
```

#### coalasce和repartition
将分区设置为指定数量,coalasce中含有一个是否shuffle的参数，该参数默认为false,在false的情形下，通过coalasce不能减少分区数，但是如果将shuffle设置为true，则可以将分区数进行增加。而这正式repartition的底层实现逻辑。

```java
    def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
        coalesce(numPartitions, shuffle = true)
    }
```

```java
  def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[3]").setAppName("CreateRDD")
        val sc = new SparkContext(conf)
        val rdd1: RDD[Int] = sc.parallelize(Array(30,50,70,60,10,20,30,30))
        // 减少分区
        println(rdd1.getNumPartitions)
        println(rdd1.coalesce(2).getNumPartitions)
        // 默认没有shuffle，所以直接增加分区不行，会失败，
        println(rdd1.coalesce(4).getNumPartitions)
        // 如果要增加分区必须shuffle
        println(rdd1.coalesce(4,true).getNumPartitions)
        // 也可以使用repartition增加分区，底层调用的coalesce
        println(rdd1.repartition(4).getNumPartitions)

    }
```
#### sortBy
很难，又很简单的一个函数，用于排序，要通过shuffle.\
简单用法，一重排序。

```java
  def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("CreateRDD")
        val sc = new SparkContext(conf)
        val rdd1: RDD[Int] = sc.parallelize(Array(30,50,70,60,10,20,30,30))
        // 降序
        rdd1.sortBy(x=>x,true).collect().foreach(println)
        // 升序
        rdd1.sortBy(x=>x,false).collect().foreach(println)
    }
```


```java
def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("CreateRDD")
        val sc = new SparkContext(conf)
        val rdd1: RDD[String] = sc.parallelize(Array("aa","bb","abc","abcde"))

        rdd1.sortBy[(Int,String)](x=>(x.length,x),false).collect().foreach(println)
        val value: RDD[String] = rdd1.sortBy(x => (x.length, x))(Ordering.Tuple2(Ordering.Int.reverse, Ordering.String.reverse), ClassTag(classOf[(Int, String)]))
        value.collect().foreach(println)
    }
```


#### pipe
管道，针对每个分区，把 RDD 中的每个数据通过管道传递给shell命令或脚本，返回输出的RDD。一个分区执行一次这个命令. 如果只有一个分区, 则执行一次命令.

#### RDD的特殊玩法

### 双value
交集差集并集拉链

```java
 def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("CreateRDD")
        val sc = new SparkContext(conf)
        val rdd1: RDD[Int] = sc.parallelize(Array(30,50,70,60,10,20,30,30))
        val rdd2: RDD[Int] = sc.parallelize(Array(30,50,7,6,1,10,20,30,30))
        val rdd3: RDD[Int] = rdd1.union(rdd2)
        // 并集
        println("并集")
        rdd3.collect().foreach(println)
        (rdd1 ++ rdd2).collect.foreach(println)
        // 交集
        println("交集")
        rdd1.intersection(rdd2).collect.foreach(println)
        // 差集，会将30减完
        println("差集")
        rdd1.subtract(rdd2).collect.foreach(println)
        // 拉链，分区数一定要一样,每个分区类的个数不一致
        println("拉链表")
       // rdd1.zip(rdd2).collect.foreach(println)
        rdd1.zipWithIndex().collect.foreach(println)
        // 只要求分区一致,利用scala的灵活
        println("灵活拉链表")
        rdd1.zipPartitions(rdd2)((it1,it2)=>{
            // 多余的扔掉
            it1.zip(it2)
            // it1和it2拉链，如果it1不够补-1，如果it2不够补-2
            it1.zipAll(it2,-1,-2)
        }).collect.foreach(println)
        // 笛卡尔积
        rdd1.cartesian(rdd2).collect().foreach(println)
    }
```








