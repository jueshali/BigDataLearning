# 自定义组件

## 自定义生产者(调用kafkaproducer)

kafka的Produce发送消息采用的时异步发送的方式，在消息发送的过程中，涉及到两个线程main线程和Sender线程。还有一个线程共享变量RecordAccumulator.main线程经过拦截器，分区器，序列化器，将消息发送给RecordAccumlator保存，RecordAccumlator的大小可以指定。之后sender从Recorder中拉取数据发送到broker.

```java
import kafka.server.KafkaConfig;
import org.apache.kafka.clients.producer.*;

import java.util.ArrayList;
import java.util.List;
import java.util.Properties;
import java.util.concurrent.ExecutionException;

/**
 * 项目名：kafka
 * 描述：自定义producer
 *
 * @author : Lpc
 * @date : 2019-12-04 09:12
 **/
public class Myproducer {

    public static void main(String[] args) throws ExecutionException, InterruptedException {
        Properties props = new Properties();
        // 配置kafka服务器
        props.put("bootstrap.servers", "hadoop101:9092");
        // 配置acks，
        props.put("acks", "all");
        // 配置错误后重发次数
        props.put("retries", 0);
        // 配置每个producer的batch的size 
        props.put("batch.size", 16384);
        // 数据为达到batch.size之后sender才会发送数据
        // 如果数据为达到batch.size，sender等待linger.time之后就会发送数据。
        props.put("linger.ms", 1);
        // 缓存地址
        props.put("buffer.memory", 33554432);
        // 序列化器
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        props.put(ProducerConfig.PARTITIONER_CLASS_CONFIG,"MyPartitioner");
        //自定义拦截器
        List<String> interceptors=new ArrayList<String>();

        interceptors.add("MyTimeInterceptor");
        interceptors.add("MyCountInterceptor");

        props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,interceptors);
        
        Producer<String, String> producer = new KafkaProducer<String, String>(props);
        // sender由于与main线程不是一个线程，所以其可以带回调函数
        // 同时也可以实现同步，同步的开启只要调用get()方法即可。
        for (int i = 0; i < 10; i++){
//            producer.send(new ProducerRecord<String, String>("hello2", Integer.toString(i), Integer.toString(i)));
            producer.send(new ProducerRecord<String, String>("hello2", Integer.toString(i), Integer.toString(i)), new Callback() {
                public void onCompletion(RecordMetadata metadata, Exception exception) {
                    if (exception!=null){
                        System.out.println("something wrong");
                    }
                    else {
                        System.out.println(metadata.partition()+"--->"+metadata.offset());
                    }
                }
            }).get();

            producer.flush();

        }
        producer.close();
    }
}
```

## 自定义消费者(调用kafkaconsumer)

```java
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;

import java.util.Arrays;
import java.util.Properties;

/**
 * 项目名：kafka
 * 描述：自定义Consumer
 *
 * @author : Lpc
 * @date : 2019-12-04 19:00
 **/
public class MyConsumer {

    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "hadoop101:9092");
        props.put("group.id", "Mytest");
        // 允许自动提交，此处可以设置为false. 如果设置为false
        props.put("enable.auto.commit", "true");
        //提交的时间间镉
        props.put("auto.commit.interval.ms", "1000");
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);
        consumer.subscribe(Arrays.asList("hello2", "hello3 "));
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(100);
            for (ConsumerRecord<String, String> record : records) {
                System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
            }
            co
        }
    }
}

```
- 当`props.put("enable.auto.commit", "true");`时为开启自动提交，提交的时间是经过一个时间间镉 `props.put("auto.commit.interval.ms", "1000");`确定，由于数据处可能发生在提交之后，因此如果处理时出现问题，将造成数据丢失，例如提交了十个数据，但才处理玩5个数据，如果此时消费者重启，那消费者将从11个数据开始消费，丢失5个数据。
- 当`props.put("enable.auto.commit", "false");`时会关闭自动提交.此时可以通过调用consumer.commitSync()或者consumer.commitAsync进行手动提交。如果设置在数据处理完成之后手动提交，如果在提交时发生异常，数据处理部分后，提交失败，那么在下次消费时会发生数据的重复消费。
- 两种方法各有利弊，可行的解决方法就是将数据的处理和offset的提交（可以提交到本地mysql数据库）做成一个事务，发生异常就backoff。当然最好的解决办法就是不用自己写的comsumer，利用flume的kafkasink不香吗？

## 自定义生产者拦截器

自定义拦截器首先要实现`ProducerInterceptor`接口，实现此接口有三个方法，configure是读取配置信息，onSend()是对消息进行拦截处理后发送，onAcknowledgement()是收到broker回复的ack请求后的响应。close（）在消费者关闭时调用。在编写好拦截器后，需要在producer中指定 `props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,interceptors);` 传入的是一个拦截器数组，main线程将按照数组存入的顺序对record进行处理。

```java
/**
 * 项目名：kafka
 * 描述：给每条记录加上时间戳
 *
 * @author : Lpc
 * @date : 2019-12-04 19:34
 **/
public class MyTimeInterceptor implements ProducerInterceptor<String,String> {
    public ProducerRecord<String, String> onSend(ProducerRecord<String, String> record) {
        String value = record.value();

        value = new Date().toString()+"-->"+value;

     return new ProducerRecord<String,String>(record.topic(),record.key(),value+record.timestamp());
    }

    public void onAcknowledgement(RecordMetadata metadata, Exception exception) {

    }

    public void close() {

    }

    public void configure(Map<String, ?> configs) {

    }
}
```
## 自定义生产者分区器

自定义分区器同样需要实现Partitioner接口，configure获取配置，close()在producer关闭时调用，partition是主要的分区实现方法。其返回一个int类型的值，该值不能大于当前topic的分区数，当前topic的可用分区数可以通过` cluster.availablePartitionsForTopic(topic).size())`获取。

```java

/**
 * 项目名：kafka
 * 描述：自定义分区器
 *
 * @author : Lpc
 * @date : 2019-12-04 19:58
 **/
public class MyPartitioner implements Partitioner {
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
        System.out.println("当前主题为---》"+topic + "其分区数为---》"  + cluster.availablePartitionsForTopic(topic).size());
        return 0;
    }
    public void close() {
    }
    public void configure(Map<String, ?> configs) {
    }
}

```
